import torch

torch.set_printoptions(precision=4)

# # ============================================================
# # ğŸ® çŒœæ•¸å­—éŠæˆ² - ç”¨æ¢¯åº¦ä¸‹é™å­¸ç¿’ï¼
# # ============================================================
# # ç›®æ¨™ï¼šæˆ‘å€‘è¨­å®šä¸€å€‹æ­£ç¢ºç­”æ¡ˆï¼ˆä¾‹å¦‚ï¼šç›®æ¨™å€¼æ˜¯ 20.0ï¼‰ã€‚
# # èµ·é»ï¼šè®“é›»è…¦éš¨æ©ŸçŒœä¸€å€‹æ•¸å­— xï¼ˆä¾‹å¦‚ï¼š5.0ï¼‰ã€‚
# # è©•ä¼°ï¼šè¨ˆç®—é›»è…¦çŒœçš„æ•¸å­—è·Ÿç›®æ¨™å·®å¤šå°‘ï¼ˆé€™å°±æ˜¯ Lossï¼ŒéŒ¯èª¤ç‡ï¼‰ã€‚
# # ä¿®æ­£ï¼šæ ¹æ“šã€Œæ–œç‡ï¼ˆæ¢¯åº¦ï¼‰ã€ï¼Œè®“é›»è…¦è‡ªå·±æ±ºå®šè¦æŠŠ x è®Šå¤§é‚„æ˜¯è®Šå°ã€‚
# # ============================================================

# print("ğŸ® çŒœæ•¸å­—éŠæˆ² - PyTorch æ¢¯åº¦ä¸‹é™ç¤ºç¯„")
# print("=" * 50)

# # 1ï¸âƒ£ è¨­å®šç›®æ¨™å€¼ï¼ˆæ­£ç¢ºç­”æ¡ˆï¼‰
# target = 20.0
# print(f"ğŸ¯ ç›®æ¨™å€¼: {target}")

# # 2ï¸âƒ£ é›»è…¦çš„åˆå§‹çŒœæ¸¬ï¼ˆéš¨æ©Ÿèµ·é»ï¼‰
# # requires_grad=True å‘Šè¨´ PyTorchï¼šã€Œè«‹è¿½è¹¤é€™å€‹è®Šæ•¸çš„æ¢¯åº¦ï¼ã€
# x = torch.tensor([5.0], requires_grad=True)
# print(f"ğŸ¤– é›»è…¦åˆå§‹çŒœæ¸¬: {x.item()}")
# print("=" * 50)

# # 3ï¸âƒ£ è¨­å®šå­¸ç¿’ç‡ï¼ˆæ¯æ¬¡èª¿æ•´çš„æ­¥ä¼å¤§å°ï¼‰
# learning_rate = 0.1

# # 4ï¸âƒ£ é–‹å§‹å­¸ç¿’ï¼ˆè¿­ä»£ 20 æ¬¡ï¼‰
# for step in range(20):
#     # è¨ˆç®— Lossï¼ˆæå¤±å‡½æ•¸ï¼‰= (çŒœæ¸¬å€¼ - ç›®æ¨™å€¼)Â²
#     # ç‚ºä»€éº¼ç”¨å¹³æ–¹ï¼Ÿå› ç‚ºï¼š
#     #   - æ°¸é æ˜¯æ­£æ•¸ï¼ˆä¸ç®¡çŒœå¤ªå¤§æˆ–å¤ªå°ï¼‰
#     #   - å·®è·è¶Šå¤§ï¼ŒLoss è¶Šå¤§ï¼ˆæ‡²ç½°æ›´é‡ï¼‰
#     loss = (x - target) ** 2

#     # åå‘å‚³æ’­ï¼šè¨ˆç®— Loss å° x çš„æ¢¯åº¦ï¼ˆæ–œç‡ï¼‰
#     # æ¢¯åº¦ = d(loss)/dx = 2 * (x - target)
#     loss.backward()

#     # å–å¾—æ¢¯åº¦å€¼
#     gradient = x.grad.item()

#     # 5ï¸âƒ£ æ›´æ–°çŒœæ¸¬å€¼ï¼ˆæ¢¯åº¦ä¸‹é™çš„æ ¸å¿ƒï¼ï¼‰
#     # æ–°çš„ x = èˆŠçš„ x - å­¸ç¿’ç‡ * æ¢¯åº¦
#     #
#     # ç‚ºä»€éº¼è¦ã€Œæ¸›ã€æ¢¯åº¦ï¼Ÿ
#     # - å¦‚æœ x < targetï¼šæ¢¯åº¦æ˜¯è² çš„ â†’ æ¸›è² æ•¸ = åŠ  â†’ x è®Šå¤§ âœ“
#     # - å¦‚æœ x > targetï¼šæ¢¯åº¦æ˜¯æ­£çš„ â†’ æ¸›æ­£æ•¸ = æ¸› â†’ x è®Šå° âœ“
#     with torch.no_grad():  # æ›´æ–°æ™‚ä¸éœ€è¦è¿½è¹¤æ¢¯åº¦
#         x -= learning_rate * x.grad

#     # æ¸…é›¶æ¢¯åº¦ï¼ˆç‚ºä¸‹ä¸€æ¬¡è¿­ä»£åšæº–å‚™ï¼‰
#     x.grad.zero_()

#     # é¡¯ç¤ºé€™ä¸€è¼ªçš„çµæœ
#     print(f"ç¬¬ {step+1:2d} è¼ª | çŒœæ¸¬: {x.item():8.4f} | Loss: {loss.item():10.4f} | æ¢¯åº¦: {gradient:8.4f}")

# print("=" * 50)
# print(f"ğŸ‰ æœ€çµ‚ç­”æ¡ˆ: {x.item():.4f}")
# print(f"ğŸ¯ ç›®æ¨™å€¼:   {target}")
# print(f"ğŸ“ èª¤å·®:     {abs(x.item() - target):.6f}")

target = torch.tensor([20.0])
x = torch.tensor([5.0], requires_grad=True)

print(f"ğŸ¯ ç›®æ¨™å€¼: {target}")
print(f"ğŸ¤– é›»è…¦åˆå§‹çŒœæ¸¬: {x.item(): .2f}")

learning_rate = 0.1  # 2.0

print("-" * 30)

for epoch in range(20):
    # Step 1: è¨ˆç®—ç›®å‰çš„éŒ¯èª¤
    loss = (x - target) ** 2

    # # Step 2: åå‘å‚³æ’­(Backward)ï¼Œè¨ˆç®—æ¢¯åº¦(æ–œç‡)
    # loss.backward()

    # # Step 3: æ›´æ–° x çš„å€¼ (æ‰‹å‹•åŸ·è¡Œæ¢¯åº¦ä¸‹é™çš„å…¬å¼)
    # # ä½¿ç”¨ torch.no_grad() ä¾†é¿å…è¿½è¹¤æ¢¯åº¦ï¼›æ˜¯ç‚ºäº†å‘Šè¨´PyTorchæ›´æ–°æ•¸å­—ï¼Œé€™ä»¶äº‹ä¸éœ€è¦è¢«è¨˜éŒ„åœ¨å›ç¨‹åœ°åœ–ä¸­
    # with torch.no_grad():
    #     x -= learning_rate * x.grad

    # æ”¹ç”¨å„ªåŒ–å™¨
    optimizer = torch.optim.SGD([x], lr=learning_rate)

    # 1. ç®—å‡ºæ¢¯åº¦
    loss.backward()

    # 2. è®“å„ªåŒ–å™¨å¹«ä½ èµ°ä¸€æ­¥ (ä»£æ›¿ x -= ...)
    optimizer.step()

    # 3. è®“å„ªåŒ–å™¨å¹«ä½ æ¸…ç©ºæ¢¯åº¦ (ä»£æ›¿ x.grad.zero_())
    optimizer.zero_grad()

    # Step 4: æ¸…ç©ºæ¢¯åº¦! (ç‚ºä¸‹ä¸€æ¬¡è¿­ä»£åšæº–å‚™)
    # x.grad.zero_()

    # if (epoch + 1) % 5 == 0:
    print(
        f"ç¬¬ {epoch+1} æ¬¡å¾ªç’° - ç›®å‰çŒœæ¸¬: {x.item():.4f}, éŒ¯èª¤ (Loss): {loss.item():.4f}"
    )
print("-" * 30)
print(f"è¨“ç·´çµæŸï¼æœ€çµ‚çµæœ: {x.item():.4f}")
